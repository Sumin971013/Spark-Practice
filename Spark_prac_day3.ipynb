{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://parkaparka.tistory.com/15?category=814878 출처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark is an open source cluster computing framework.\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "Originally developed at the University of California, Berkeley's AMPLab,\n",
      "the Spark codebase was later donated to the Apache Software Foundation,\n",
      "which has maintained it since.\n",
      "Spark provides an interface for programming entire clusters with\n",
      "implicit data parallelism and fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for value in myrdd.collect():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting #of words from document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd_spark = myrdd.filter(lambda line : \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'Spark provides an interface for programming entire clusters with']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_spark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = myrdd.filter(lambda line: \"framework\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd2 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_example.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data Big data', 'apache spark Apache Spark', 'pyspark Pyspark pySpark']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_map = myrdd2.map(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data', 'Big', 'data'],\n",
       " ['apache', 'spark', 'Apache', 'Spark'],\n",
       " ['pyspark', 'Pyspark', 'pySpark']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'Big', 'data']\n",
      "['apache', 'spark', 'Apache', 'Spark']\n",
      "['pyspark', 'Pyspark', 'pySpark']\n"
     ]
    }
   ],
   "source": [
    "for i in wc_map:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_fmap = myrdd2.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'data',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'pyspark',\n",
       " 'Pyspark',\n",
       " 'pySpark']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = myrdd.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework.',\n",
       " '아파치',\n",
       " '스파크는',\n",
       " '오픈',\n",
       " '소스',\n",
       " '클러스터',\n",
       " '컴퓨팅',\n",
       " '프레임워크이다.',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " 'Originally',\n",
       " 'developed',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California,',\n",
       " \"Berkeley's\",\n",
       " 'AMPLab,',\n",
       " 'the',\n",
       " 'Spark',\n",
       " 'codebase',\n",
       " 'was',\n",
       " 'later',\n",
       " 'donated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Apache',\n",
       " 'Software',\n",
       " 'Foundation,',\n",
       " 'which',\n",
       " 'has',\n",
       " 'maintained',\n",
       " 'it',\n",
       " 'since.',\n",
       " 'Spark',\n",
       " 'provides',\n",
       " 'an',\n",
       " 'interface',\n",
       " 'for',\n",
       " 'programming',\n",
       " 'entire',\n",
       " 'clusters',\n",
       " 'with',\n",
       " 'implicit',\n",
       " 'data',\n",
       " 'parallelism',\n",
       " 'and',\n",
       " 'fault-tolerance.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_wcex.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap(x):\n",
    "    return [i.upper() for i in x]\n",
    "\n",
    "\n",
    "def decap(x):\n",
    "    return [i.lower() for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd1 = wcRdd.map(lambda x: x.split(\" \"))\n",
    "wcRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "capWcRdd = wcRdd1.map(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capWcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "decapWcRdd = wcRdd1.map(decap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decapWcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = wcRdd1.map(lambda x: [i.upper() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = wcRdd1.map(lambda x:[i.lower() for i in x])\n",
    "test2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
