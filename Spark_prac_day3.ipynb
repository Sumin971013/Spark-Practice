{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://parkaparka.tistory.com/15?category=814878 출처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark is an open source cluster computing framework.\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "Originally developed at the University of California, Berkeley's AMPLab,\n",
      "the Spark codebase was later donated to the Apache Software Foundation,\n",
      "which has maintained it since.\n",
      "Spark provides an interface for programming entire clusters with\n",
      "implicit data parallelism and fault-tolerance.\n",
      "Apache Spark is an open source cluster computing framework.\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "Originally developed at the University of California, Berkeley's AMPLab,\n",
      "the Spark codebase was later donated to the Apache Software Foundation,\n",
      "which has maintained it since.\n",
      "Spark provides an interface for programming entire clusters with\n",
      "implicit data parallelism and fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for value in myrdd.collect():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting #of words from document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd_spark = myrdd.filter(lambda line : \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'Spark provides an interface for programming entire clusters with']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'Spark provides an interface for programming entire clusters with']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_spark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = myrdd.filter(lambda line: \"framework\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd2 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_example.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data Big data', 'apache spark Apache Spark', 'pyspark Pyspark pySpark']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['big data Big data', 'apache spark Apache Spark', 'pyspark Pyspark pySpark']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_map = myrdd2.map(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data', 'Big', 'data'],\n",
       " ['apache', 'spark', 'Apache', 'Spark'],\n",
       " ['pyspark', 'Pyspark', 'pySpark']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['big', 'data', 'Big', 'data'],\n",
       " ['apache', 'spark', 'Apache', 'Spark'],\n",
       " ['pyspark', 'Pyspark', 'pySpark']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'Big', 'data']\n",
      "['apache', 'spark', 'Apache', 'Spark']\n",
      "['pyspark', 'Pyspark', 'pySpark']\n",
      "['big', 'data', 'Big', 'data']\n",
      "['apache', 'spark', 'Apache', 'Spark']\n",
      "['pyspark', 'Pyspark', 'pySpark']\n"
     ]
    }
   ],
   "source": [
    "for i in wc_map:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_fmap = myrdd2.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'data',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'pyspark',\n",
       " 'Pyspark',\n",
       " 'pySpark']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'data',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'pyspark',\n",
       " 'Pyspark',\n",
       " 'pySpark']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = myrdd.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework.',\n",
       " '아파치',\n",
       " '스파크는',\n",
       " '오픈',\n",
       " '소스',\n",
       " '클러스터',\n",
       " '컴퓨팅',\n",
       " '프레임워크이다.',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " 'Originally',\n",
       " 'developed',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California,',\n",
       " \"Berkeley's\",\n",
       " 'AMPLab,',\n",
       " 'the',\n",
       " 'Spark',\n",
       " 'codebase',\n",
       " 'was',\n",
       " 'later',\n",
       " 'donated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Apache',\n",
       " 'Software',\n",
       " 'Foundation,',\n",
       " 'which',\n",
       " 'has',\n",
       " 'maintained',\n",
       " 'it',\n",
       " 'since.',\n",
       " 'Spark',\n",
       " 'provides',\n",
       " 'an',\n",
       " 'interface',\n",
       " 'for',\n",
       " 'programming',\n",
       " 'entire',\n",
       " 'clusters',\n",
       " 'with',\n",
       " 'implicit',\n",
       " 'data',\n",
       " 'parallelism',\n",
       " 'and',\n",
       " 'fault-tolerance.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework.',\n",
       " '아파치',\n",
       " '스파크는',\n",
       " '오픈',\n",
       " '소스',\n",
       " '클러스터',\n",
       " '컴퓨팅',\n",
       " '프레임워크이다.',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " '아파치',\n",
       " '스파크',\n",
       " 'Originally',\n",
       " 'developed',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California,',\n",
       " \"Berkeley's\",\n",
       " 'AMPLab,',\n",
       " 'the',\n",
       " 'Spark',\n",
       " 'codebase',\n",
       " 'was',\n",
       " 'later',\n",
       " 'donated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Apache',\n",
       " 'Software',\n",
       " 'Foundation,',\n",
       " 'which',\n",
       " 'has',\n",
       " 'maintained',\n",
       " 'it',\n",
       " 'since.',\n",
       " 'Spark',\n",
       " 'provides',\n",
       " 'an',\n",
       " 'interface',\n",
       " 'for',\n",
       " 'programming',\n",
       " 'entire',\n",
       " 'clusters',\n",
       " 'with',\n",
       " 'implicit',\n",
       " 'data',\n",
       " 'parallelism',\n",
       " 'and',\n",
       " 'fault-tolerance.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_wcex.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap(x):\n",
    "    return [i.upper() for i in x]\n",
    "\n",
    "\n",
    "def decap(x):\n",
    "    return [i.lower() for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd1 = wcRdd.map(lambda x: x.split(\" \"))\n",
    "wcRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "capWcRdd = wcRdd1.map(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capWcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "decapWcRdd = wcRdd1.map(decap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decapWcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'DaTa'],\n",
       " ['BiG', 'data'],\n",
       " ['BIg', 'DAta'],\n",
       " ['aPAche', 'spArk'],\n",
       " ['aPache', 'SpaRk'],\n",
       " ['ApaChe', 'spArk'],\n",
       " ['ApaCHE', 'Spark']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['big data',\n",
       " 'big DaTa',\n",
       " 'BiG data',\n",
       " 'BIg DAta',\n",
       " 'aPAche spArk',\n",
       " 'aPache SpaRk',\n",
       " 'ApaChe spArk',\n",
       " 'ApaCHE Spark']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = wcRdd1.map(lambda x: [i.upper() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['BIG', 'DATA'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK'],\n",
       " ['APACHE', 'SPARK']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['big', 'data'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark'],\n",
       " ['apache', 'spark']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = wcRdd1.map(lambda x:[i.lower() for i in x])\n",
    "test2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i','me','my','myself','we','our','ours','ourselves','you','yours','I','be',\n",
    "            'is','am','are','for','the','a','at','this','their','that','to','by','in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_bigdata_stopwords_ex.txt\n",
      "Overwriting data/ds_bigdata_stopwords_ex.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_bigdata_stopwords_ex.txt\n",
    "Hello my name is park.\n",
    "This is a text which is written by Sumin.\n",
    "I am going to talk about the big data.\n",
    "Big data has 3 features calle velocity, variey,volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(os.path.join(\"data\",\"ds_bigdata_stopwords_ex.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is park.\\n',\n",
       " 'This is a text which is written by Sumin.\\n',\n",
       " 'I am going to talk about the big data.\\n',\n",
       " 'Big data has 3 features calle velocity, variey,volume.\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello my name is park.\\n',\n",
       " 'This is a text which is written by Sumin.\\n',\n",
       " 'I am going to talk about the big data.\\n',\n",
       " 'Big data has 3 features calle velocity, variey,volume.\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(os.path.join(\"data\",\"ds_bigdata_stopwords_ex.txt\"))\n",
    "for sent in f.readlines():\n",
    "    for word in sent.split():\n",
    "        \n",
    "        if word not in stopwords:\n",
    "            \n",
    "            if word not in d:\n",
    "                d[word] = 1\n",
    "            \n",
    "            else:\n",
    "                d[word] = d[word]+1\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 1,\n",
       " 'name': 1,\n",
       " 'park.': 1,\n",
       " 'This': 1,\n",
       " 'text': 1,\n",
       " 'which': 1,\n",
       " 'written': 1,\n",
       " 'Sumin.': 1,\n",
       " 'going': 1,\n",
       " 'talk': 1,\n",
       " 'about': 1,\n",
       " 'big': 1,\n",
       " 'data.': 1,\n",
       " 'Big': 1,\n",
       " 'data': 1,\n",
       " 'has': 1,\n",
       " '3': 1,\n",
       " 'features': 1,\n",
       " 'calle': 1,\n",
       " 'velocity,': 1,\n",
       " 'variey,volume.': 1}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Hello': 1,\n",
       " 'name': 1,\n",
       " 'park.': 1,\n",
       " 'This': 1,\n",
       " 'text': 1,\n",
       " 'which': 1,\n",
       " 'written': 1,\n",
       " 'Sumin.': 1,\n",
       " 'going': 1,\n",
       " 'talk': 1,\n",
       " 'about': 1,\n",
       " 'big': 1,\n",
       " 'data.': 1,\n",
       " 'Big': 1,\n",
       " 'data': 1,\n",
       " 'has': 1,\n",
       " '3': 1,\n",
       " 'features': 1,\n",
       " 'calle': 1,\n",
       " 'velocity,': 1,\n",
       " 'variey,volume.': 1}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is park.',\n",
       " 'This is a text which is written by Sumin.',\n",
       " 'I am going to talk about the big data.',\n",
       " 'Big data has 3 features calle velocity, variey,volume.']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello my name is park.',\n",
       " 'This is a text which is written by Sumin.',\n",
       " 'I am going to talk about the big data.',\n",
       " 'Big data has 3 features calle velocity, variey,volume.']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_stopwords_ex.txt\"))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sword_rdd = rdd1.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'park.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'text',\n",
       " 'which',\n",
       " 'is',\n",
       " 'written',\n",
       " 'by',\n",
       " 'Sumin.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'the',\n",
       " 'big',\n",
       " 'data.',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'has',\n",
       " '3',\n",
       " 'features',\n",
       " 'calle',\n",
       " 'velocity,',\n",
       " 'variey,volume.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'park.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'text',\n",
       " 'which',\n",
       " 'is',\n",
       " 'written',\n",
       " 'by',\n",
       " 'Sumin.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'the',\n",
       " 'big',\n",
       " 'data.',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'has',\n",
       " '3',\n",
       " 'features',\n",
       " 'calle',\n",
       " 'velocity,',\n",
       " 'variey,volume.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sword_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd_stop1 = sword_rdd.filter(lambda x: x.lower() not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'name',\n",
       " 'park.',\n",
       " 'text',\n",
       " 'which',\n",
       " 'written',\n",
       " 'Sumin.',\n",
       " 'going',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'big',\n",
       " 'data.',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'has',\n",
       " '3',\n",
       " 'features',\n",
       " 'calle',\n",
       " 'velocity,',\n",
       " 'variey,volume.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'name',\n",
       " 'park.',\n",
       " 'text',\n",
       " 'which',\n",
       " 'written',\n",
       " 'Sumin.',\n",
       " 'going',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'big',\n",
       " 'data.',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'has',\n",
       " '3',\n",
       " 'features',\n",
       " 'calle',\n",
       " 'velocity,',\n",
       " 'variey,volume.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_stop1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd_stop2 = myrdd_stop1.map(lambda x: (x,1)).groupByKey().mapValues(sum).sortByKey(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 1),\n",
       " ('Big', 1),\n",
       " ('Hello', 1),\n",
       " ('Sumin.', 1),\n",
       " ('about', 1),\n",
       " ('big', 1),\n",
       " ('calle', 1),\n",
       " ('data', 1),\n",
       " ('data.', 1),\n",
       " ('features', 1),\n",
       " ('going', 1),\n",
       " ('has', 1),\n",
       " ('name', 1),\n",
       " ('park.', 1),\n",
       " ('talk', 1),\n",
       " ('text', 1),\n",
       " ('variey,volume.', 1),\n",
       " ('velocity,', 1),\n",
       " ('which', 1),\n",
       " ('written', 1)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('3', 1),\n",
       " ('Big', 1),\n",
       " ('Hello', 1),\n",
       " ('Sumin.', 1),\n",
       " ('about', 1),\n",
       " ('big', 1),\n",
       " ('calle', 1),\n",
       " ('data', 1),\n",
       " ('data.', 1),\n",
       " ('features', 1),\n",
       " ('going', 1),\n",
       " ('has', 1),\n",
       " ('name', 1),\n",
       " ('park.', 1),\n",
       " ('talk', 1),\n",
       " ('text', 1),\n",
       " ('variey,volume.', 1),\n",
       " ('velocity,', 1),\n",
       " ('which', 1),\n",
       " ('written', 1)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd_stop2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
